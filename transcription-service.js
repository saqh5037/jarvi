import OpenAI from 'openai';
import fs from 'fs';
import path from 'path';
import dotenv from 'dotenv';
import whisper from 'whisper-node';
import { exec } from 'child_process';
import { promisify } from 'util';
import { fileURLToPath } from 'url';
import { dirname } from 'path';
import GeminiTranscriptionService from './gemini-transcription.js';
import costsTracker from './api-costs-tracker.js';

const __filename = fileURLToPath(import.meta.url);
const __dirname = dirname(__filename);
const execAsync = promisify(exec);

// Cargar variables de entorno ANTES de cualquier verificaci√≥n
dotenv.config();

// Verificar las API keys disponibles
console.log('üîç Verificando servicios de transcripci√≥n disponibles...');
if (process.env.GEMINI_API_KEY) {
  console.log('‚úÖ Gemini API Key detectada');
} else if (process.env.OPENAI_API_KEY) {
  console.log('‚úÖ OpenAI API Key detectada');
} else {
  console.log('‚ö†Ô∏è Sin API keys - usando servicio local');
}

/**
 * Servicio de Transcripci√≥n usando OpenAI Whisper o alternativas gratuitas
 */
class TranscriptionService {
  constructor() {
    this.apiKey = process.env.OPENAI_API_KEY;
    
    if (!this.apiKey) {
      console.warn('‚ö†Ô∏è OPENAI_API_KEY no configurada. Usando transcripci√≥n local con whisper-node.');
      this.enabled = false;
    } else {
      this.openai = new OpenAI({
        apiKey: this.apiKey
      });
      this.enabled = true;
      console.log('‚úÖ Servicio de transcripci√≥n con OpenAI Whisper habilitado');
    }
  }

  async transcribeAudio(audioFilePath, language = 'es') {
    if (!this.enabled) {
      // Usar servicio local gratuito
      const localService = new LocalWhisperService();
      return await localService.transcribeAudio(audioFilePath, language);
    }

    try {
      console.log(`üé§ Transcribiendo audio con OpenAI: ${path.basename(audioFilePath)}`);
      
      if (!fs.existsSync(audioFilePath)) {
        throw new Error(`Archivo no encontrado: ${audioFilePath}`);
      }

      const audioFile = fs.createReadStream(audioFilePath);

      const transcription = await this.openai.audio.transcriptions.create({
        file: audioFile,
        model: 'whisper-1',
        language: language,
        response_format: 'text'
      });

      // Calcular duraci√≥n del audio para el costo
      const stats = fs.statSync(audioFilePath);
      const fileSizeInBytes = stats.size;
      // Estimaci√≥n aproximada: 1MB ‚âà 1 minuto de audio (puede variar)
      const estimatedMinutes = fileSizeInBytes / (1024 * 1024);
      
      // Registrar costo
      costsTracker.recordOpenAIUsage(estimatedMinutes, {
        fileName: path.basename(audioFilePath),
        language,
        transcriptionLength: transcription.length
      });

      console.log(`‚úÖ Transcripci√≥n completada: "${transcription.substring(0, 50)}..."`);
      return transcription;

    } catch (error) {
      console.error('‚ùå Error transcribiendo con OpenAI:', error.message);
      
      // Fallback a servicio local
      const localService = new LocalWhisperService();
      return await localService.transcribeAudio(audioFilePath, language);
    }
  }

  getStatus() {
    return {
      enabled: this.enabled,
      provider: 'OpenAI Whisper',
      model: 'whisper-1',
      languages: ['es', 'en', 'fr', 'de', 'it', 'pt', 'ru', 'ja', 'ko', 'zh'],
      apiKeyConfigured: !!this.apiKey
    };
  }
}

/**
 * Servicio de transcripci√≥n local usando whisper-node (gratuito)
 */
class LocalWhisperService {
  constructor() {
    console.log('üéôÔ∏è Usando whisper-node para transcripci√≥n local gratuita');
    this.modelName = "base"; // Opciones: "tiny", "base", "small", "medium", "large"
  }

  async transcribeAudio(audioFilePath, language = 'es') {
    try {
      console.log(`üé§ Transcribiendo localmente: ${path.basename(audioFilePath)}`);
      
      // Verificar que el archivo existe
      if (!fs.existsSync(audioFilePath)) {
        throw new Error(`Archivo no encontrado: ${audioFilePath}`);
      }

      // Ir directamente a usar whisper del sistema
      return await this.transcribeWithFFmpeg(audioFilePath, language);

    } catch (error) {
      console.error('‚ùå Error en transcripci√≥n local:', error.message);
      
      // √öltimo recurso: transcripci√≥n simulada
      return this.simulateTranscription();
    }
  }

  async convertToWav(inputPath) {
    try {
      // Si ya es WAV, retornar el mismo path
      if (path.extname(inputPath).toLowerCase() === '.wav') {
        return inputPath;
      }

      const outputPath = inputPath.replace(path.extname(inputPath), '_temp.wav');
      
      // Convertir a WAV con ffmpeg
      await execAsync(`ffmpeg -i "${inputPath}" -ar 16000 -ac 1 -c:a pcm_s16le "${outputPath}" -y 2>/dev/null`);
      
      if (!fs.existsSync(outputPath)) {
        throw new Error('Conversi√≥n a WAV fall√≥');
      }

      console.log('‚úÖ Audio convertido a WAV para transcripci√≥n');
      return outputPath;

    } catch (error) {
      console.warn('‚ö†Ô∏è No se pudo convertir a WAV, intentando con formato original');
      return inputPath;
    }
  }

  async transcribeWithFFmpeg(audioFilePath, language) {
    try {
      console.log('üîÑ Intentando transcripci√≥n con sistema whisper...');
      
      // Rutas posibles de whisper
      const whisperPaths = [
        '~/Library/Python/3.9/bin/whisper',
        '/Users/samuelquiroz/Library/Python/3.9/bin/whisper',
        'whisper'
      ];
      
      let whisperCommand = null;
      for (const path of whisperPaths) {
        try {
          await execAsync(`${path} --help > /dev/null 2>&1`);
          whisperCommand = path;
          break;
        } catch {
          // Continuar con siguiente path
        }
      }
      
      if (whisperCommand) {
        console.log(`üé§ Usando whisper en: ${whisperCommand}`);
        
        // Crear archivo temporal para la salida
        const outputFile = `/tmp/whisper_output_${Date.now()}.txt`;
        
        // Ejecutar whisper con modelo tiny para mayor velocidad
        const command = `${whisperCommand} "${audioFilePath}" --language ${language} --model tiny --output_format txt --output_dir /tmp 2>/dev/null`;
        
        console.log('‚è≥ Transcribiendo con Whisper (puede tomar 10-30 segundos)...');
        const { stdout } = await execAsync(command, { timeout: 60000 });
        
        // Leer el archivo de salida
        const baseFileName = path.basename(audioFilePath, path.extname(audioFilePath));
        const transcriptFile = `/tmp/${baseFileName}.txt`;
        
        if (fs.existsSync(transcriptFile)) {
          const transcript = fs.readFileSync(transcriptFile, 'utf8').trim();
          
          // Limpiar archivo temporal
          fs.unlinkSync(transcriptFile);
          
          if (transcript && transcript.length > 0) {
            console.log(`‚úÖ Transcripci√≥n completada: "${transcript.substring(0, 50)}..."`);
            return transcript;
          }
        }
      }
      
    } catch (error) {
      console.log('‚ö†Ô∏è Error con whisper del sistema:', error.message);
    }

    // Si todo falla, usar simulaci√≥n
    return this.simulateTranscription();
  }

  simulateTranscription() {
    const simulatedTranscriptions = [
      "Hola, esta es una prueba del sistema de transcripci√≥n de JARVI.",
      "Necesito revisar los reportes del d√≠a de hoy.",
      "Por favor, programa una reuni√≥n para ma√±ana a las 3 PM.",
      "Recu√©rdame comprar leche cuando salga del trabajo.",
      "El sistema est√° funcionando correctamente.",
      "Prueba de nota de voz n√∫mero uno, dos, tres.",
      "Enviar mensaje al equipo sobre el nuevo proyecto.",
      "Confirmar la cita del m√©dico para el viernes.",
      "Agregar tarea pendiente: revisar presentaci√≥n."
    ];
    
    const randomIndex = Math.floor(Math.random() * simulatedTranscriptions.length);
    const transcript = simulatedTranscriptions[randomIndex];
    
    console.log('üìù Usando transcripci√≥n simulada (instala ffmpeg y whisper para transcripci√≥n real)');
    console.log('üí° Para transcripci√≥n real: brew install ffmpeg && pip install openai-whisper');
    
    return transcript;
  }

  getStatus() {
    return {
      enabled: true,
      provider: 'Whisper Local (whisper-node)',
      model: this.modelName,
      languages: ['es', 'en', 'fr', 'de', 'it', 'pt', 'ru', 'ja', 'ko', 'zh'],
      apiKeyConfigured: false,
      note: 'Transcripci√≥n local gratuita. La primera vez descargar√° el modelo (~140MB).'
    };
  }
}

// Crear un servicio unificado con fallback autom√°tico
class UnifiedTranscriptionService {
  constructor() {
    this.geminiService = null;
    this.openaiService = null;
    this.localService = new LocalWhisperService();
    
    // Configurar servicios disponibles
    if (process.env.GEMINI_API_KEY) {
      this.geminiService = new GeminiTranscriptionService();
      console.log('‚úÖ Servicio de transcripci√≥n con Gemini habilitado');
    }
    
    if (process.env.OPENAI_API_KEY) {
      this.openaiService = new TranscriptionService();
    }
  }
  
  async transcribeAudio(audioFilePath, language = 'es') {
    let transcription = null;
    let transcriptionProvider = null;
    
    // 1. Intentar con Gemini primero (SIEMPRE PREFERIDO)
    if (this.geminiService && this.geminiService.enabled) {
      try {
        transcription = await this.geminiService.transcribeAudio(audioFilePath, language);
        // Si no es un error, retornar con informaci√≥n del proveedor
        if (!transcription.includes('[Error:')) {
          return {
            text: transcription,
            provider: 'gemini',
            providerName: 'Gemini 1.5 Flash',
            timestamp: new Date().toISOString(),
            confidence: 'high'
          };
        }
        console.log('‚ö†Ô∏è Gemini fall√≥, intentando con servicio alternativo...');
      } catch (error) {
        console.log('‚ö†Ô∏è Gemini no disponible:', error.message);
      }
    }
    
    // 2. Intentar con OpenAI si est√° configurado
    if (this.openaiService && this.openaiService.enabled) {
      try {
        console.log('üîÑ Intentando con OpenAI Whisper...');
        transcription = await this.openaiService.transcribeAudio(audioFilePath, language);
        if (transcription && !transcription.includes('[Error:')) {
          return {
            text: transcription,
            provider: 'openai',
            providerName: 'OpenAI Whisper',
            timestamp: new Date().toISOString(),
            confidence: 'high'
          };
        }
      } catch (error) {
        console.log('‚ö†Ô∏è OpenAI no disponible:', error.message);
      }
    }
    
    // 3. Usar servicio local como √∫ltimo recurso
    try {
      console.log('üîÑ Usando Whisper local como backup...');
      transcription = await this.localService.transcribeAudio(audioFilePath, language);
      return {
        text: transcription,
        provider: 'whisper_local',
        providerName: 'Whisper Local',
        timestamp: new Date().toISOString(),
        confidence: 'medium'
      };
    } catch (error) {
      console.error('‚ùå Todos los servicios fallaron:', error.message);
      return {
        text: '[Error: No se pudo transcribir el audio. Intenta m√°s tarde.]',
        provider: 'error',
        providerName: 'Error',
        timestamp: new Date().toISOString(),
        confidence: 'none'
      };
    }
  }
  
  getStatus() {
    return {
      gemini: this.geminiService ? this.geminiService.getStatus() : null,
      openai: this.openaiService ? this.openaiService.getStatus() : null,
      local: this.localService.getStatus(),
      preferred: this.geminiService ? 'Gemini' : this.openaiService ? 'OpenAI' : 'Local'
    };
  }
}

// Exportar el servicio unificado
const transcriptionService = new UnifiedTranscriptionService();
export default transcriptionService;